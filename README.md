<div>

# ã€AAAI 2024 ğŸ”¥ã€‘CLIP åœ¨æ–‡æœ¬æè¿°è¡Œäººæ£€ç´¢ä¸­çš„ç»éªŒç ”ç©¶
[![Paper](http://img.shields.io/badge/Paper-AAAI_2024-Green.svg)](https://ojs.aaai.org/index.php/AAAI/article/view/27801)
[![Paper](http://img.shields.io/badge/Paper-arxiv.2308.10045-FF6B6B.svg)](https://arxiv.org/abs/2308.10045)
</div>

æœ¬ä»“åº“æä¾› TBPS-CLIPï¼ˆText-based Person Searchï¼‰çš„å®˜æ–¹ PyTorch å®ç°ï¼Œå¯¹åº”è®ºæ–‡ã€ŠAn Empirical Study of CLIP for Text-based Person Searchã€‹ã€‚

å¦‚æœä½ å¯¹æ–‡æœ¬æè¿°è¡Œäººæ£€ç´¢æ–¹å‘æ„Ÿå…´è¶£ï¼Œæ¬¢è¿åŒæ—¶é˜…è¯»æˆ‘ä»¬çš„å°ä¼™ä¼´é¡¹ç›®ï¼š
- ã€ACM MM 2023ã€‘[Text-based Person Search without Parallel Image-Text Data](https://arxiv.org/abs/2305.12964)
- ã€IJCAI 2023ã€‘[RaSa: Relation and Sensitivity Aware Representation Learning for Text-based Person Search](https://arxiv.org/abs/2305.13653)
- ã€ICASSP 2022ã€‘[Learning Semantic-Aligned Feature Representation for Text-based Person Search](https://arxiv.org/abs/2112.06714)

## è¯´æ˜
æ›´å¤šå®éªŒç»†èŠ‚ä¸æ‹“å±•ç»“æœå·²é™„åœ¨ [arXiv ç‰ˆæœ¬](https://arxiv.org/abs/2308.10045)çš„é™„å½•ä¸­ã€‚

## é¡¹ç›®æ¦‚è§ˆ
æˆ‘ä»¬é‡æ–°å®¡è§†äº† [CLIP](https://arxiv.org/abs/2103.00020) åœ¨æ•°æ®å¢å¹¿å’ŒæŸå¤±å‡½æ•°è®¾è®¡ä¸­çš„å…³é”®ç­–ç•¥ï¼Œæå‡ºäº†é¢å‘æ–‡æœ¬æè¿°è¡Œäººæ£€ç´¢çš„å¼ºå¤§åŸºçº¿ TBPS-CLIPã€‚

<img src="image/intro.png" width="550">

## ç¯å¢ƒå‡†å¤‡
- é»˜è®¤å®éªŒç¯å¢ƒï¼š4 å¼  NVIDIA A40ï¼ˆ48GBï¼‰GPUï¼ŒCUDA 11.7ã€‚
- æ¨èä½¿ç”¨ Python 3.9+ï¼Œå¹¶é€šè¿‡ `requirements.txt` å®‰è£…ä¾èµ–ï¼š

```sh
pip install -r requirements.txt
```

## æ•°æ®ä¸é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½
1. æ•°æ®é›†ï¼š
   - ä» [é“¾æ¥](https://github.com/ShuangLI59/Person-Search-with-Natural-Language-Description) ä¸‹è½½ CUHK-PEDESã€‚
   - ä» [é“¾æ¥](https://github.com/zifyloo/SSAN) ä¸‹è½½ ICFG-PEDESã€‚
   - ä» [é“¾æ¥](https://github.com/NjtechCVLab/RSTPReid-Dataset) ä¸‹è½½ RSTPReidã€‚
2. æ ‡æ³¨æ–‡ä»¶ï¼šåœ¨ [Google Drive](https://drive.google.com/file/d/1C5bgGCABtuzZMaa2n4Sc0qclUvZ-mqG9/view?usp=drive_link) ä¸­è·å–æ•´ç†å¥½çš„ JSON æ ‡æ³¨ã€‚
3. é¢„è®­ç»ƒæƒé‡ï¼š
   - åŸç‰ˆ CLIPï¼šä¸‹è½½ OpenAI å‘å¸ƒçš„ [ViT-B/16 æƒé‡](https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt)ã€‚
   - AltCLIPï¼šä½¿ç”¨ Hugging Face Hub ä¸­çš„ `BAAI/AltCLIP` ç³»åˆ—æ¨¡å‹ï¼Œè¯¦ç»†è¯´æ˜è§ä¸‹æ–‡ã€‚

## é…ç½®æ–‡ä»¶
åœ¨ `config/config.yaml` ä¸ `config/s.config.yaml` ä¸­å¡«å†™ï¼š
- `data.annotation_file`ï¼šæ•°æ®æ ‡æ³¨æ–‡ä»¶è·¯å¾„ã€‚
- `data.image_root`ï¼šå›¾åƒç›®å½•ã€‚
- `model.clip_pretrained` æˆ– `model.altclip_pretrained`ï¼šå¯¹åº”æ¨¡å‹æƒé‡è·¯å¾„ï¼ˆAltCLIP æ”¯æŒæœ¬åœ°ç¼“å­˜ç›®å½•ï¼‰ã€‚
- å…¶ä½™è¶…å‚ï¼ˆå­¦ä¹ ç‡ã€batch size ç­‰ï¼‰å¯æ ¹æ®è®¾å¤‡èµ„æºè°ƒæ•´ã€‚

## è®­ç»ƒä¸è¯„ä¼°
ä½¿ç”¨ `torchrun` å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒï¼š

```sh
CUDA_VISIBLE_DEVICES=0,1,2,3 \
 torchrun --rdzv_id=3 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 --nproc_per_node=4 \
 main.py
```

è‹¥æƒ³ä½¿ç”¨ç®€åŒ–é…ç½®ï¼Œå¯åŠ ä¸Š `--simplified` é€‰é¡¹ï¼š

```sh
CUDA_VISIBLE_DEVICES=0,1,2,3 \
 torchrun --rdzv_id=3 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 --nproc_per_node=4 \
 main.py --simplified
```

## AltCLIP å¤šè¯­è¨€æ‰©å±•
AltCLIP æ¨¡å‹ç°å·²ä¸ TBPS-CLIP å®Œæ•´é›†æˆï¼Œå¯æä¾›ä¸­æ–‡ã€è‹±æ–‡åŠå¤šç§è¯­è¨€æè¿°çš„ç»Ÿä¸€ç¼–ç èƒ½åŠ›ã€‚æœ¬èŠ‚ä»å·®å¼‚è¯´æ˜ã€ä¾èµ–é…ç½®ã€è®­ç»ƒæ”¹åŠ¨å’Œè°ƒè¯•è„šæœ¬å››ä¸ªæ–¹é¢ç»™å‡ºè¯¦ç»†æŒ‡å—ã€‚

### ä¸åŸç‰ˆ CLIP çš„æ ¸å¿ƒå·®å¼‚
- **æ–‡æœ¬ç¼–ç å™¨å‡çº§ä¸º XLM-Rã€‚** AltCLIP é‡‡ç”¨ XLM-RoBERTa ä½œä¸ºæ–‡æœ¬éª¨å¹²ï¼Œåœ¨ `model/altclip_adapter.py` ä¸­ç”±é€‚é…å™¨å¯¹é½åˆ°åŸ `CLIP` ç±»çš„æ¥å£ï¼ŒåŸæœ‰æŸå¤±å‡½æ•°ã€ä¼˜åŒ–é€»è¾‘å¯æ— ç¼å¤ç”¨ã€‚
- **è¡¨å¾ç»´åº¦æå‡è‡³ 768ã€‚** AltCLIP é»˜è®¤è¾“å‡º 768 ç»´ç‰¹å¾ï¼Œéœ€å°†é…ç½®ä¸­çš„ `model.embed_dim`ã€`model.projection_dim` ç­‰å­—æ®µåŒæ­¥æ”¹ä¸º 768ï¼›è‹¥ä½¿ç”¨è‡ªå®šä¹‰å¤´éƒ¨ï¼Œè¯·ç¡®ä¿çº¿æ€§å±‚è¾“å…¥å°ºå¯¸ä¸€è‡´ã€‚
- **åˆ†è¯å™¨æ”¹ä¸º Hugging Face å®ç°ã€‚** `text_utils/tokenizer.py` ä¸­æ–°å¢ `AltCLIPTokenizer`ï¼ŒåŸºäº `transformers.AutoTokenizer` è°ƒç”¨ `BAAI/AltCLIP` çš„è¯è¡¨ã€‚å¯ç”¨ AltCLIP æ—¶ä¼šè‡ªåŠ¨å±è”½åŸæœ¬ä¾èµ– BPE çš„æ•°æ®å¢å¼ºé€‰é¡¹ã€‚
- **é¢„å¤„ç†ä¸å›¾åƒå½’ä¸€åŒ–ç•¥æœ‰å·®å¼‚ã€‚** AltCLIP ä½¿ç”¨è‡ªå¸¦çš„ `AltCLIPProcessor` å®Œæˆå›¾åƒ resizeã€ä¸­å¿ƒè£å‰ªä¸å½’ä¸€åŒ–ï¼›é€‚é…å™¨ä¼šåœ¨ `build_altclip_clip` ä¸­è¿”å› `processor`ï¼Œæ–¹ä¾¿åœ¨è‡ªå®šä¹‰è„šæœ¬ä¸­å…±äº«åŒä¸€å¥—é¢„å¤„ç†æµç¨‹ã€‚
- **æƒé‡æ¥æºä¸º Hugging Face Hubã€‚** é»˜è®¤è”ç½‘çŠ¶æ€ä¸‹é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ï¼›è‹¥åœ¨ç¦»çº¿ç¯å¢ƒï¼Œè¯·å…ˆç”¨ `transformers-cli download BAAI/AltCLIP` æˆ–è„šæœ¬ç¼“å­˜ç›¸å…³æƒé‡ï¼Œå¹¶åœ¨é…ç½®ä¸­æŒ‡å®šæœ¬åœ°ç›®å½•ã€‚

### å®‰è£…ä¸é…ç½®æ­¥éª¤
1. å®‰è£…é¢å¤–ä¾èµ–ï¼š
   ```sh
   pip install transformers>=4.35.0 sentencepiece safetensors
   ```
2. åœ¨ `config/config.yaml` ä¸­å¯ç”¨ AltCLIPï¼š
   ```yaml
   model:
     backbone: altclip          # æ–°å¢å­—æ®µï¼Œå–å€¼ clip æˆ– altclip
     embed_dim: 768
     tokenizer_type: altclip
     altclip_pretrained: "BAAI/AltCLIP"
   experiment:
     use_altclip_processor: true
     mlm: false                 # AltCLIP ä¸æ”¯æŒåŸæœ‰ BPE-MLM å¢å¼º
   ```
3. å¦‚æœéœ€è¦è‡ªå®šä¹‰å›¾åƒå¤§å°æˆ–åˆ†è¾¨ç‡ï¼Œè¯·åŒæ­¥ä¿®æ”¹ `data.image_size`ï¼Œå¹¶ç¡®è®¤ `AltCLIPProcessor` çš„ resize æ’å€¼ç­–ç•¥æ»¡è¶³éœ€æ±‚ã€‚

### å¿«é€Ÿä½“éªŒè„šæœ¬
- è¿è¡Œ `misc/quick_start_altclip.py` å¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…å®Œæˆä¸€æ¬¡ä¸­è‹±æ–‡æè¿°çš„æ­£è´Ÿæ ·æœ¬å‰å‘æµ‹è¯•ï¼š
  ```sh
  python misc/quick_start_altclip.py --device cuda:0 --text "çº¢è‰²ä¸Šè¡£çš„å¥³ç”Ÿ" "a man wearing a blue coat"
  ```
- è„šæœ¬ä¼šæ‰“å°å›¾æ–‡ç›¸ä¼¼åº¦çŸ©é˜µä¸ logit_scaleã€logit_bias ç­‰å…³é”®å‚æ•°ï¼ŒéªŒè¯ AltCLIP æ˜¯å¦åŠ è½½æˆåŠŸã€‚
- å¦‚éœ€å¤ç°è®ºæ–‡è®­ç»ƒæµç¨‹ï¼Œå¯å°†è„šæœ¬ä¸­çš„ `build_altclip_clip` å¯¼å…¥è‡ªå·±çš„æ•°æ®ç®¡çº¿ï¼Œåªéœ€æ›¿æ¢æ¨¡å‹æ„å»ºéƒ¨åˆ†å³å¯ã€‚

### åœ¨è®­ç»ƒæµæ°´çº¿ä¸­åˆ‡æ¢è‡³ AltCLIP
1. è°ƒæ•´ `options.py` ä¸­çš„å‘½ä»¤è¡Œå‚æ•°æˆ– YAML é…ç½®ï¼Œä½¿ `--backbone altclip` è¢«è§£æã€‚
2. åœ¨ `main.py` ä¸­è°ƒç”¨ `build_altclip_clip`ï¼Œæ¥æ”¶ `clip_model, tokenizer, processor` ä¸‰å…ƒç»„ã€‚
3. æ•°æ®åŠ è½½é˜¶æ®µä½¿ç”¨ `processor(images=..., return_tensors="pt")` å¤„ç†å›¾åƒï¼Œå¹¶å°†æ–‡æœ¬æ‰¹æ¬¡ä¼ å…¥æ–°çš„ tokenizerã€‚
4. è®­ç»ƒ/è¯„ä¼°è„šæœ¬å…¶ä½™éƒ¨åˆ†ä¿æŒä¸å˜ï¼›è‹¥ä½¿ç”¨è‡ªå®šä¹‰å¤´éƒ¨ï¼Œéœ€è¦æ ¹æ® 768 ç»´ç‰¹å¾é‡æ–°åˆå§‹åŒ–çº¿æ€§å±‚ã€‚

### ç¦»çº¿éƒ¨ç½²å»ºè®®
- ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æå‰ä¸‹è½½æƒé‡åŠåˆ†è¯å™¨ï¼š
  ```sh
  python -c "from transformers import AutoModel, AutoTokenizer; AutoModel.from_pretrained('BAAI/AltCLIP', cache_dir='./hf_cache'); AutoTokenizer.from_pretrained('BAAI/AltCLIP', cache_dir='./hf_cache')"
  ```
- å°†é…ç½®ä¸­çš„ `altclip_pretrained`ã€`tokenizer_cache_dir` æŒ‡å‘ `./hf_cache`ï¼Œå³å¯åœ¨æ— ç½‘ç»œç¯å¢ƒä¸­å¤ç°è®­ç»ƒä¸æ¨ç†ã€‚

## æ¨¡å‹æƒé‡
| æ¨¡å‹ | CUHK-PEDES | ICFG-PEDES | RSTPReid |
|:--:|:--:|:--:|:--:|
| **TBPS-CLIP (ViT-B/16)** | [ä¸‹è½½](https://drive.google.com/file/d/1m_3pKanUWHQHeJ-zt-QeRXs7bmay-U5P/view?usp=drive_link) | [ä¸‹è½½](https://drive.google.com/file/d/1az4z5b_ADXR7DcysPB5giOl52LjWDCSu/view?usp=drive_link) | [ä¸‹è½½](https://drive.google.com/file/d/1qMUAsH-1lzkWUFQsUvUKTY0J6ZuGkYd6/view?usp=drive_link) |
| **ç®€åŒ–ç‰ˆ TBPS-CLIP (ViT-B/16)** | [ä¸‹è½½](https://drive.google.com/file/d/1W5oFZK9WNHMfy0OOaYQBzPsP1LZR80bT/view?usp=drive_link) | [ä¸‹è½½](https://drive.google.com/file/d/1UoLd-MQ8tYJ7YPgCbh3nVSVYnJ9a_TG5/view?usp=drive_link) | [ä¸‹è½½](https://drive.google.com/file/d/18zlc3q3Sze5rx3TqcfEeZEjrQXUTpcQF/view?usp=drive_link) |

## é¸£è°¢
- [CLIP](https://arxiv.org/abs/2103.00020) â€”â€” TBPS-CLIP çš„æ ¸å¿ƒæ¶æ„æ¥æºã€‚

## å¼•ç”¨
å¦‚æœæœ¬ä»“åº“å¯¹ä½ çš„ç ”ç©¶æˆ–é¡¹ç›®æœ‰æ‰€å¸®åŠ©ï¼Œæ¬¢è¿ star ğŸŒŸ å¹¶å¼•ç”¨ ğŸ“‘ æˆ‘ä»¬çš„è®ºæ–‡ï¼š

```
@inproceedings{cao2024empirical,
  title={An Empirical Study of CLIP for Text-Based Person Search},
  author={Cao, Min and Bai, Yang and Zeng, Ziyin and Ye, Mang and Zhang, Min},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={1},
  pages={465--473},
  year={2024}
}
```

## è®¸å¯è¯
æœ¬é¡¹ç›®åŸºäº MIT License å¼€æºï¼Œå…·ä½“æ¡æ¬¾è§ä»“åº“æ ¹ç›®å½•çš„ `LICENSE` æ–‡ä»¶ã€‚
