device: 5

misc:
  seed: 1

experiment:
  # image
  input_resolution: [224, 224]
  simclr_mlp: [768, 256, 768]
  simclr_temperature: 0.1
  # text
  dropout: 0.05
  eda_alpha: 0.05
  back_trans: true
  backtrans_p: 0.1
  text_length: 77
  # mix
  mixgen: false
  mixgen_type: cat  # ori or cat
  mixgen_p: 0.1
  mixgen_ratio: 0.1
  mvs_image: false  # 关闭多视图采样以节省显存

  # loss
  nitc_ratio: 1.0
  ####
  ss: false  # 关闭 Self-supervised learning 以节省显存
  ss_ratio: 0.4
  ####
  ritc: true
  ritc_eps: 1.0e-2
  ritc_ratio: 1.0
  ####
  mlm: false
  mlm_ratio: 1.0
  cmt_depth: 4 # cross modal transformer self attn layers
  ####
  citc: false  # 关闭 Cross-modal Instance Token Contrastive 以节省显存和加速训练
  citc_lambda1: 0.25
  citc_lambda2: 0.25
  citc_ratio: 0.1
  ####
  id: false
  id_ratio: 1.0

schedule:
  lr: 5.0e-5  # 降低学习率到 1e-5
  epoch: 10  # 训练 10 个 epoch
  epoch_warmup: 1
  lr_start: 1.0e-6
  lr_end: 5.0e-6
  weight_decay: 0.02
  betas: [0.9, 0.98]
  eps: 1.0e-8
  ratio_factor: 5.0

model:
  ckpt_type: original_clip  # original_clip / saved
  saved_path: 'ckpts/baseline_224_224/CUHK-PEDES'
  checkpoint: '/data1/model/AltCLIP'
  use_gather: true
  softlabel_ratio: 0.5
  embed_dim: 768
  vocab_size: 49408

text_encoder: '/data1/model/AltCLIP'

mlm:
  is_mlm: false
  mask_prob: 0.15
  max_masks: 10
  skipgram_prb: 0.2
  skipgram_size: 3

log:
  print_period: 50

data:
  batch_size: 44  # 提升到 40（显存有余量）
  test_batch_size: 64
  num_workers: 8
  languages: ['en', 'ch', 'de', 'fr']

distributed:
  backend: nccl
  url: 'env://'

anno_dir: '/data1/zxy/MM-TBPS/datasets/CUHK/'
image_dir: '/data1/dataset/cuhkpedes/imgs'